\section{Experimental Setup}

\begin{tiny}
\begin{table*}[!htpb]
\caption{Events used in this paper (part 1).  
         Values in parenthesis are {\tt perf} raw event numbers.}
\label{table:events1}
\centering
\scriptsize
\input{tables/events_part1}
\hspace{0.2in}$^{\dagger}$This counter does not work on Atom N270 or 230.
\end{table*}
\end{tiny}

\begin{table*}[!htpb]
\caption{Events used in this paper (part 2).  
         Values in parenthesis are {\tt perf} raw event numbers.}
\label{table:events2}
\centering
\scriptsize
\input{tables/events_part2}
\hspace{0.2in}$^{\ddagger}$ Event support dropped in 6/2010
Intel Vol3B, interacts poorly with HyperThreading
\end{table*}

\begin{table*}[!htpb]
\caption{Events used in this paper (part 3).  
         Values in parenthesis are {\tt perf} raw event numbers.}
\label{table:events2}
\centering
\scriptsize
\input{tables/events_part3}
\end{table*}

Processor vendors make no guarantees about determinism or
counter accuracy; any limitations must be determined
experimentally.  
We investigate 
multiple x86\_64 
implementations to see if any of the performance events can
provide deterministic events with no overcount, suitable for
applications such as
parallel deterministic execution.  
We also investigate the availability of such events on
other platforms.

\subsection{External Sources of Non-Determinism}

Measuring exact event counts can be difficult due to various 
external sources of variation found in a typical system, including
operating system interaction~\cite{mcguire+:rtlw09},
program layout~\cite{weaver+:iiswc08,mytkowicz+:asplos09},
measurement overhead~\cite{zaparanuks+:ispass09},
multi-processor variation~\cite{alameldeen+:hpca03}, and
hardware implementation details~\cite{weaver+:iiswc08,weaver:thesis10}.
In our experiments we attempt to avoid these sources of variability by
carefully controlling our test environment.

Benchmarks often have internal sources of non-determinism
that are inherent in their design, usually unintentionally.
If a program depends on the time, pointer values, or I/O input,
then the application can take unpredictable paths through its
codebase.
Even benchmarks designed to give repeatable results,
such as SPEC CPU, can vary in subtle ways due to a changing
operating system environment~\cite{weaver+:iiswc08}.
We carefully construct our test-cases to avoid 
these sources of variation as much as possible.

\subsection{Our Custom Assembly Benchmark}

Analysis of performance counter accuracy is difficult;
it requires exact knowledge of all executing instructions
and their effects on a system.
This precludes using existing benchmarks written in high level languages 
as the resulting binaries are compiler dependent and no
``known'' overall instruction count is available.
Compilers rarely use the full complement of available 
processor opcodes, leaving many unexplored corner cases.
Total aggregate event measurements over large benchmarks can
make major divergences from estimated totals visible, 
but the root causes can be nearly impossible 
to discover.  
Counts can vary due to complex interactions
deep within a program and can be perturbed by debugging.

We avoid the variation inherent in high-level benchmarks
by writing a large assembly language benchmark. 
This microbenchmark has over 200 million dynamic instructions,
which is larger than the interval size used in many computer 
architecture investigations.
The benchmark attempts to exercise most x86\_64 instructions
while having no outside dependencies (by calling 
operating system syscalls directly, as in 
our previous code density investigation~\cite{weaver+:iccd09}).

Due to the CISC nature of the x86 architecture it is difficult to 
make a completely comprehensive test.  
We exercise most integer, x87 floating point,
MMX, and SSE instructions (up to and including SSE3).  
We attempt to use various combinations of 
register accesses, operand sizes (single byte accesses up through
128-bit SSE),
memory accesses, and the wide variety of x86 addressing modes.
Sections of the code are looped many thousands of times to 
make anomalies stand out in the overall instruction count
and to allow binary searches for extra counts.
The complete annotated source for the microbenchmark is available 
from our website:\\
\url{http://www.eece.maine.edu/~vweaver/projects/deterministic/}
% Add code sample?

We measure userspace events generated by our benchmark alone;
the operating system provides process-specific counts by saving
and restoring the counter values at context switch time
and the CPU performance monitoring unit (PMU) differentiates
between events happening in user and kernel domains.
There are many other conceivable sources of variation, such as
crossing cache-line boundaries, crossing page boundaries, 
unaligned instruction fetches, unaligned memory accesses, etc.
We have not found these to affect event counts.

%
% Events
%

\subsection{Events}

Modern processors have hundreds of available performance events
(a full list can be found in the various vendor's architectural 
manuals~\cite{intel:asdvol3,amd:fam10bkdg}).
We limit our search to those described as counting retired or
committed instructions.

In general the following types of retired 
instruction counts are available:
\begin{itemize}
\item {\bf total retired instructions}
\item {\bf retired branches} (total or conditional), % Used by ReVirt?
\item {\bf retired loads and stores},
and
\item {\bf retired floating point and SSE}.
\end{itemize}

In addition, many processors provide retired counts
of unusual instructions, such as {\tt fxch},
{\tt cpuid}, move operations, serializing instructions,
memory barriers, multiplies and divides, and not-taken branches.  
While these
are useful when analyzing specific program bottlenecks, they
are less useful for large-scale validation work.
Other retired events, such as retired $\mu$ops,
are unsuitable because they are speculative and 
implementation dependent.

Tables~\ref{table:events1},~\ref{table:events2}, and~\ref{table:events3}
list the names of the events for which
we provide detailed results.

\subsection{The Experiments}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Machines used in this study.  
}
\label{table:machines_used}
\centering
\input{tables/machines}
\end{table}

We ran our assembly benchmark ten times each on eleven different x86\_64 
machines as shown in Table~\ref{table:machines_used}.
We compare the results of our benchmarks against an expected value 
determined via code inspection.  
Due to circumstances beyond our control the test machines are 
running different Linux kernel revisions;
we ran tests of 
various kernels and performance counter implementations on the same machine
and found that the different kernel infrastructures
have no impact on userspace-only aggregate counter results.
We use the {\tt perf} tool on systems that support the {\tt perf\_events}
interface, and the {\tt pfmon}
tool systems using {\tt perfmon2}~\cite{eranian:ols06}.

The {\tt perf} tool only supports a small number of common ``generic'' 
events; many events have to be specified using a
raw event code.  We use the libpfm4 library to determine these codes.
We run {\tt perf} as follows:

\begin{footnotesize}
{\tt perf stat -e r5001c0:u ./retired\_instructions}
\end{footnotesize}

\noindent
In this example {\tt r5001c0} corresponds to the Core2 
{\tt RETIRED\_LOADS} event
and the {\tt :u} mask specifies
we only care about user-space (not kernel) counts.

The {\tt pfmon} utility included with perfmon2
has a much more user-friendly interface that uses proper event names.
It is run like this:

\begin{footnotesize}
{\tt pfmon -e RETIRED\_LOADS ./retired\_instructions}
\end{footnotesize}

